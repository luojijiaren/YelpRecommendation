{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17153 4733\n"
     ]
    }
   ],
   "source": [
    "def present(idn):\n",
    "    d={}\n",
    "    for i,idn in enumerate(idn):\n",
    "        d[idn]=i\n",
    "    return d  \n",
    "#/my_data/\n",
    "train=pd.read_csv('/my_data/train.csv',index_col=0)\n",
    "validate=pd.read_csv('/my_data/validate.csv',index_col=0)\n",
    "\n",
    "# train=train[:50000]\n",
    "# validate=validate[:20000]\n",
    "\n",
    "users=sorted(list(set(train.index).union(set(validate.index))))\n",
    "nb_users=len(users)\n",
    "business=sorted(list(set(train['business_id']).union(set(validate['business_id']))))\n",
    "nb_business=len(business)\n",
    "print(nb_users,nb_business)\n",
    "dic=present(business)\n",
    "# Converting the data into an array with users in lines and movies in columns\n",
    "\n",
    "def convert(data,batch_size):\n",
    "    #new_data = []\n",
    "    #data=data.iloc[0:(nb_users//batch_size)*batch_size,:]\n",
    "    for i,id_users in enumerate(users):\n",
    "        \n",
    "        id_business = data['business_id'][data.index == id_users]\n",
    "        ind=list(map(lambda x:dic[x],id_business))\n",
    "        #id_business = list(map(lambda x:business.index(x),l))\n",
    "        id_ratings = data['stars'][data.index == id_users]\n",
    "        ratings = np.zeros(nb_business)\n",
    "        ratings[ind] = id_ratings\n",
    "        if i%batch_size==0:\n",
    "            if i!=0:\n",
    "                yield np.array(U)\n",
    "            U=[list(ratings)]\n",
    "        else:\n",
    "            U.append(list(ratings))\n",
    "        #new_data.append(list(ratings))\n",
    "    #return new_data\n",
    "        \n",
    "    \n",
    "train_set = convert(train,500)\n",
    "val_set = convert(validate,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LFM:\n",
    "    \n",
    "    def __init__(self,train_set,nb_users,nb_business,factor,batch_size,iter_num = 20,alpha = 0.1,Lambda = 0.1,epsilon = 0.01):\n",
    "        '''\n",
    "        initiate parameters\n",
    "        '''\n",
    "        self.train_set = train_set\n",
    "        self.nb_users=nb_users\n",
    "        self.nb_business=nb_business\n",
    "        self.factor = factor\n",
    "        self.batch_size=batch_size\n",
    "        self.batchs=nb_users//batch_size\n",
    "        self.alpha = alpha\n",
    "        self.iter_num = iter_num\n",
    "        self.Lambda = Lambda\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    \n",
    "    def build(self,train):\n",
    "        \n",
    "\n",
    "        #initiate latent factor matrix\n",
    "        self.decompose_p = tf.Variable(tf.ones([self.nb_business,self.factor]))\n",
    "        self.decompose_q = tf.Variable(tf.ones([self.factor,self.nb_users]))    \n",
    "        \n",
    "        # Placeholders\n",
    "\n",
    "        self.u_ind = tf.placeholder('int32', shape=[self.batch_size])\n",
    "        #i = tf.placeholder('int32')\n",
    "        self.y_rate = tf.placeholder('float32', shape=[self.batch_size, self.nb_business])\n",
    "        self.y_mask = tf.placeholder('float32', shape=[self.batch_size, self.nb_business])\n",
    "        #batch_q = tf.placeholder('float32', shape=[self.batch_size, self.factor])\n",
    "\n",
    "        batch_q=tf.gather(tf.transpose(self.decompose_q),self.u_ind)\n",
    "        self.pred_y_rate = tf.matmul(batch_q, tf.transpose(self.decompose_p))\n",
    "        loss_m = tf.squared_difference(self.y_rate, self.pred_y_rate)\n",
    "        self.loss = tf.reduce_sum(loss_m * self.y_mask)+self.Lambda*(tf.reduce_sum(tf.square(self.decompose_p))+tf.reduce_sum(tf.square(batch_q)))\n",
    "        optimizer = tf.train.AdamOptimizer(self.alpha)\n",
    "        train_op = optimizer.minimize(self.loss)\n",
    "                                                      \n",
    "\n",
    "        #training\n",
    "        config = tf.ConfigProto() \n",
    "        config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "        self.sess = tf.Session(config=config)\n",
    "        self.sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "    \n",
    "        old_error = float('inf')\n",
    "        \n",
    "        tot_loss = 0.0\n",
    "        for epoch in range(self.iter_num):\n",
    "            train_set = convert(train,500)\n",
    "            t = time.time()\n",
    "            \n",
    "            for i,U in enumerate(train_set):\n",
    "                #with self.sess.as_default():\n",
    "                #    batch_q=tf.transpose(self.decompose_q).eval()[i*self.batch_size:(i+1)*self.batch_size,:]  \n",
    "                u_ind=list(range(i*self.batch_size,(i+1)*self.batch_size))\n",
    "                mask=deepcopy(U)\n",
    "                mask[mask!=0]=1\n",
    "                feed={self.y_rate:U,self.y_mask:mask,self.u_ind:u_ind}\n",
    "                _, loss = self.sess.run([train_op, self.loss], feed_dict = feed)\n",
    "                tot_loss += loss\n",
    "                #print (i)\n",
    "                avg_loss = tot_loss / (epoch*self.batchs+i+1)\n",
    "                if i%10==0:\n",
    "                    print (\"Epoch %d\\tLoss\\t%.2f\\tTime %dmin\" \\\n",
    "                    % (epoch, avg_loss, (time.time()-t)))\n",
    "            \n",
    "\n",
    "\n",
    "        print (\"Recommender is built!\")\n",
    "        \n",
    "    def test(self,val_set):\n",
    "        \"\"\"\n",
    "        :return performance on test set (Mean Square Root Error)\n",
    "        \"\"\"\n",
    "        tot_loss = 0.0\n",
    "        for i,U in enumerate(val_set):\n",
    "            u_ind=list(range(i*self.batch_size,(i+1)*self.batch_size))\n",
    "            mask=deepcopy(U)\n",
    "            mask[mask!=0]=1\n",
    "            feed={self.y_rate:U,self.y_mask:mask,self.u_ind:u_ind}\n",
    "            loss = self.sess.run(self.loss, feed_dict = feed)\n",
    "            tot_loss += loss\n",
    "        return tot_loss / self.batchs\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Epoch 0\tLoss\t3637474.00\tTime 6min\n",
      "Epoch 0\tLoss\t1439393.33\tTime 65min\n",
      "Epoch 0\tLoss\t820657.54\tTime 124min\n",
      "Epoch 0\tLoss\t597815.37\tTime 183min\n",
      "Epoch 1\tLoss\t534907.07\tTime 5min\n",
      "Epoch 1\tLoss\t420359.29\tTime 64min\n",
      "Epoch 1\tLoss\t346825.19\tTime 123min\n",
      "Epoch 1\tLoss\t294995.75\tTime 182min\n",
      "Epoch 2\tLoss\t278560.82\tTime 5min\n",
      "Epoch 2\tLoss\t244076.87\tTime 64min\n",
      "Epoch 2\tLoss\t218073.18\tTime 123min\n",
      "Epoch 2\tLoss\t197199.28\tTime 182min\n",
      "Epoch 3\tLoss\t190067.36\tTime 5min\n",
      "Epoch 3\tLoss\t173580.61\tTime 64min\n",
      "Epoch 3\tLoss\t159919.78\tTime 123min\n",
      "Epoch 3\tLoss\t148455.88\tTime 182min\n",
      "Epoch 4\tLoss\t144447.69\tTime 5min\n",
      "Epoch 4\tLoss\t134920.75\tTime 64min\n",
      "Epoch 4\tLoss\t126623.63\tTime 123min\n",
      "Epoch 4\tLoss\t119385.65\tTime 183min\n",
      "Epoch 5\tLoss\t116795.67\tTime 5min\n",
      "Epoch 5\tLoss\t110536.29\tTime 64min\n",
      "Epoch 5\tLoss\t104951.08\tTime 124min\n",
      "Epoch 5\tLoss\t100001.43\tTime 183min\n",
      "Epoch 6\tLoss\t98221.10\tTime 5min\n",
      "Epoch 6\tLoss\t93827.28\tTime 65min\n",
      "Epoch 6\tLoss\t89845.43\tTime 124min\n",
      "Epoch 6\tLoss\t86231.68\tTime 182min\n",
      "Epoch 7\tLoss\t84907.85\tTime 5min\n",
      "Epoch 7\tLoss\t81643.44\tTime 64min\n",
      "Epoch 7\tLoss\t78667.08\tTime 124min\n",
      "Epoch 7\tLoss\t75947.26\tTime 183min\n",
      "Epoch 8\tLoss\t74941.84\tTime 5min\n",
      "Epoch 8\tLoss\t72426.27\tTime 65min\n",
      "Epoch 8\tLoss\t70136.89\tTime 124min\n",
      "Epoch 8\tLoss\t68011.41\tTime 183min\n",
      "Epoch 9\tLoss\t67226.07\tTime 5min\n",
      "Epoch 9\tLoss\t65230.32\tTime 64min\n",
      "Epoch 9\tLoss\t63410.13\tTime 123min\n",
      "Epoch 9\tLoss\t61717.32\tTime 182min\n",
      "Epoch 10\tLoss\t61078.13\tTime 5min\n",
      "Epoch 10\tLoss\t59453.75\tTime 64min\n",
      "Epoch 10\tLoss\t57998.55\tTime 123min\n",
      "Epoch 10\tLoss\t56663.32\tTime 182min\n",
      "Epoch 11\tLoss\t56136.89\tTime 5min\n",
      "Epoch 11\tLoss\t54796.86\tTime 64min\n",
      "Epoch 11\tLoss\t53620.06\tTime 123min\n",
      "Epoch 11\tLoss\t52538.10\tTime 182min\n",
      "Epoch 12\tLoss\t52132.22\tTime 5min\n",
      "Epoch 12\tLoss\t51016.84\tTime 64min\n",
      "Epoch 12\tLoss\t50056.64\tTime 123min\n",
      "Epoch 12\tLoss\t49162.80\tTime 182min\n",
      "Epoch 13\tLoss\t48820.11\tTime 5min\n",
      "Epoch 13\tLoss\t47873.77\tTime 64min\n",
      "Epoch 13\tLoss\t47127.97\tTime 123min\n",
      "Epoch 13\tLoss\t46379.00\tTime 182min\n",
      "Epoch 14\tLoss\t46119.36\tTime 5min\n",
      "Epoch 14\tLoss\t45339.51\tTime 64min\n",
      "Epoch 14\tLoss\t44776.87\tTime 123min\n",
      "Epoch 14\tLoss\t44203.22\tTime 182min\n",
      "Epoch 15\tLoss\t44001.98\tTime 5min\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "sess=tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "lfm=LFM(train_set,nb_users,nb_business,factor=50,batch_size=500,iter_num = 30,alpha = 0.1)\n",
    "\n",
    "lfm.build(train)\n",
    "print (lfm.decompose_p)\n",
    "print (lfm.decompose_q)\n",
    "\n",
    "loss=lfm.test(val_set)\n",
    "print(loss)\n",
    "# ex = range(len(lfm.delta_error))\n",
    "# plt.figure(1)\n",
    "# plt.plot(ex,lfm.delta_error)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LFM' object has no attribute 'get_feed_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6387eae047e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlfm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-be9f670c122c>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, val_set)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mfeed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_rate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_mask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mu_ind\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mu_ind\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feed_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0mtot_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtot_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LFM' object has no attribute 'get_feed_dict'"
     ]
    }
   ],
   "source": [
    "loss=lfm.test(val_set)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
